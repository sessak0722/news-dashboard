# unified_dashboard.py â€” ì§‘ê³„â†’CSVâ†’ëŒ€ì‹œë³´ë“œ (NAVER Ã— GDELT)
# + ì•ˆì „ê°€ë“œ/â€˜ì²˜ìŒ ë“±ì¥â€™ ë²„ê·¸í”½ìŠ¤/ì‹ ì„ ë„ë°°ì§€/í—¬ìŠ¤ì²´í¬ í¬í•¨
# ì‹¤í–‰: streamlit run unified_dashboard.py

import os, time, math, json, hashlib, requests, random, re
from datetime import datetime, timedelta, timezone, date
from dateutil import parser as dtparser
from dotenv import load_dotenv
from urllib.parse import quote, urlparse
from pathlib import Path

import numpy as np
import pandas as pd
import streamlit as st
from dataclasses import dataclass

# â”€â”€ ì¶”ê°€: ì‹¤ì‹œê°„ ì£¼ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import yfinance as yf
except Exception:
    yf = None  # ì„¤ì¹˜ ì „ì—ë„ ëŒ€ì‹œë³´ë“œê°€ ì£½ì§€ ì•Šë„ë¡

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ë‰´ìŠ¤ ì–¸ê¸‰ ê¸‰ë“± í†µí•© ëŒ€ì‹œë³´ë“œ", layout="wide")
load_dotenv()
CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
KST = timezone(timedelta(hours=9))

ENTITIES_CSV = "entities.csv"
OUT_CSV = "cross_batch_counts.csv"
DOMAIN_WEIGHTS_JSON = "domain_weights.json"
KR_HOLIDAY_CSV = os.getenv("KR_HOLIDAY_CSV", "").strip()  # (ì„ íƒ) YYYY-MM-DD í•œ ì¤„ í•˜ë‚˜

# 429 íšŒí”¼ íŒŒë¼ë¯¸í„°
NAVER_CALL_DELAY_SEC = float(os.getenv("NAVER_CALL_DELAY_SEC", "0.2"))
NAVER_MAX_RETRY = int(os.getenv("NAVER_MAX_RETRY", "5"))
NAVER_BACKOFF_BASE = float(os.getenv("NAVER_BACKOFF_BASE", "0.8"))
NAVER_BACKOFF_CAP = float(os.getenv("NAVER_BACKOFF_CAP", "8.0"))

# ìºì‹œ ë””ë ‰í„°ë¦¬
CACHE_DIR = Path(".cache"); CACHE_NAVER = CACHE_DIR / "naver"; CACHE_GDELT = CACHE_DIR / "gdelt"
for p in (CACHE_NAVER, CACHE_GDELT): p.mkdir(parents=True, exist_ok=True)

# requests ì„¸ì…˜
SESSION = requests.Session()
ADAPTER = requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=20, max_retries=2)
SESSION.mount("http://", ADAPTER); SESSION.mount("https://", ADAPTER)
SESSION.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µìš© ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def today_tag(): return datetime.now(KST).strftime("%Y%m%d")

def cache_key(*parts) -> str:
    raw = "||".join(str(p) for p in parts); return hashlib.sha1(raw.encode("utf-8")).hexdigest()

def cache_get(path: Path, max_age_hours: int):
    if not path.exists(): return None
    try:
        age = datetime.now(KST) - datetime.fromtimestamp(path.stat().st_mtime, KST)
        if age > timedelta(hours=max_age_hours): return None
        with path.open("r", encoding="utf-8") as f: return json.load(f)
    except Exception: return None

def cache_put(path: Path, obj):
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        with path.open("w", encoding="utf-8") as f: json.dump(obj, f, ensure_ascii=False)
    except Exception: pass

def strip_html(text: str) -> str:
    if not text: return ""
    text = re.sub(r"</?b>", "", text); text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"&[^;\s]+;", " ", text); text = re.sub(r"\s+", " ", text).strip()
    return text

def short_summary(text: str, limit: int = 140) -> str:
    s = strip_html(text); return (s[:limit] + "â€¦") if len(s) > limit else s

# â˜…â˜…â˜… TZ ì•ˆì „ ìœ í‹¸(NEW): tz-aware/naive í˜¼ì¬ ì¸ë±ìŠ¤ í†µì¼ íŒ¨ì¹˜
def to_naive_kst(idx) -> pd.DatetimeIndex:
    """
    tz-aware â†’ Asia/Seoulë¡œ ë³€í™˜ í›„ tzì •ë³´ ì œê±°
    tz-naive â†’ ê·¸ëŒ€ë¡œ DatetimeIndexë¡œ ì •ê·œí™”
    """
    _idx = pd.DatetimeIndex(idx)
    if getattr(_idx, "tz", None) is not None:
        try:
            _idx = _idx.tz_convert(KST).tz_localize(None)
        except Exception:
            _idx = _idx.tz_localize(None)
    return _idx

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê±°ë˜ì¼ ìº˜ë¦°ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False, ttl=12*60*60)
def load_kr_holidays(csv_path: str) -> set[date]:
    if not csv_path or not os.path.exists(csv_path): return set()
    try:
        s = pd.read_csv(csv_path, header=None, names=["d"], dtype=str)["d"].dropna().str.strip()
        return set(pd.to_datetime(s).dt.date.tolist())
    except Exception:
        return set()

def business_days_index(start: date, end: date, holidays: set[date]) -> pd.DatetimeIndex:
    # tz-awareë¡œ ë§Œë“¤ì—ˆë‹¤ê°€ tz ì œê±° â†’ naive KST ì¶•
    all_days = pd.date_range(start=start, end=end, freq="D", tz=KST).tz_convert(None)
    bdays = all_days[all_days.weekday < 5]
    if holidays:
        mask = ~bdays.date.astype("O").isin(holidays)
        bdays = bdays[mask]
    return pd.DatetimeIndex(bdays)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì—”í‹°í‹° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_entities(path: str):
    if not os.path.exists(path):
        st.error(f"{path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € entities.csvë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."); st.stop()
    _df = None
    for enc in ("utf-8-sig", "cp949", "utf-8"):
        try: _df = pd.read_csv(path, encoding=enc); break
        except Exception: continue
    if _df is None:
        st.error("entities.csv ì¸ì½”ë”© ì˜¤ë¥˜: UTF-8(ë˜ëŠ” CSV UTF-8)ë¡œ ì €ì¥ í›„ ì¬ì‹œë„"); st.stop()
    _df.columns = [str(c).strip().lower() for c in _df.columns]
    def colget(cols):
        for c in cols:
            if c in _df.columns: return c
        return None
    aliases = {
        "name_kr": ["name_kr","name","íšŒì‚¬ëª…","ê¸°ì—…","ì¢…ëª©ëª…"],
        "name_en": ["name_en","english","ì˜ë¬¸ëª…"],
        "display": ["display","í‘œì‹œëª…","ë³´ì—¬ì¤„ì´ë¦„","label","title"],
        "type":    ["type","ë¶„ë¥˜","ì¹´í…Œê³ ë¦¬"],
        "enabled": ["enabled","use","ì‚¬ìš©","active"],
        "ticker":  ["ticker","í‹°ì»¤","symbol","ì¢…ëª©ì½”ë“œ","code"],
        "aliases": ["aliases","ë³„ì¹­","ë™ì˜ì–´","aka","synonyms"],
    }
    for k, al in aliases.items():
        if k not in _df.columns:
            c = colget(al); _df[k] = _df[c] if c else ""
    def truthy(x): return 1 if str(x).strip().lower() in ("1","true","y","yes","on") else 0
    _df["enabled"] = _df["enabled"].map(truthy).fillna(1).astype(int)
    for c in ["name_kr","name_en","display","ticker","type","aliases"]:
        _df[c] = _df[c].fillna("").astype(str).str.strip()
    _df.loc[_df["type"]=="","type"] = "company"
    _df.loc[_df["display"]=="","display"] = _df["name_kr"].where(_df["name_kr"]!="", _df["name_en"])
    _df = _df[_df["enabled"]==1].copy()
    rows = []
    for _, r in _df.iterrows():
        if not (str(r.get("name_kr","")).strip() or str(r.get("name_en","")).strip() or str(r.get("display","")).strip()):
            continue
        rows.append({
            "type": r.get("type","company"),
            "name_kr": r.get("name_kr",""),
            "name_en": r.get("name_en",""),
            "display": r.get("display",""),
            "ticker": r.get("ticker",""),
            "aliases": r.get("aliases",""),
        })
    if not rows:
        st.error("entities.csvì— enabled=1ì¸ ì—”í‹°í‹°ê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()
    return rows

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë„ë©”ì¸ ê°€ì¤‘ì¹˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False, ttl=12*60*60)
def load_domain_weights(path: str) -> dict:
    try:
        if not os.path.exists(path): return {}
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        return {str(k).strip().lower(): float(v) for k, v in obj.items()}
    except Exception:
        return {}

def get_domain_weight(domain: str, wmap: dict) -> float:
    if not domain: return 1.0
    d = domain.lower()
    return float(wmap.get(d, 1.0))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NAVER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _sleep_with_jitter(base: float):
    time.sleep(base + random.random() * 0.15)

def naver_news_search_all(query: str, max_items: int = 100, sort: str = "date", cache_hours: int = 6):
    if not CLIENT_ID or not CLIENT_SECRET:
        raise RuntimeError("NAVER í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤(.env).")
    display = min(100, max(10, max_items))
    params = {"query": query, "display": display, "start": 1, "sort": sort}
    key = cache_key(today_tag(), query, display, sort)
    cpath = CACHE_NAVER / f"{key}.json"
    cached = cache_get(cpath, cache_hours)
    if cached is not None: return cached
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    last_err = None
    for attempt in range(1, NAVER_MAX_RETRY + 1):
        try:
            r = SESSION.get(url, headers=headers, params=params, timeout=15)
            if r.status_code == 429:
                ra = r.headers.get("Retry-After")
                wait = float(ra) if ra and re.match(r"^\d+(\.\d+)?$", ra) else min(NAVER_BACKOFF_CAP, NAVER_BACKOFF_BASE * (2 ** (attempt - 1)))
                _sleep_with_jitter(wait); last_err = requests.HTTPError(f"429 Too Many Requests (attempt {attempt})")
                continue
            if 500 <= r.status_code < 600:
                wait = min(NAVER_BACKOFF_CAP, NAVER_BACKOFF_BASE * (2 ** (attempt - 1)))
                _sleep_with_jitter(wait); last_err = requests.HTTPError(f"{r.status_code} Server Error (attempt {attempt})")
                continue
            r.raise_for_status()
            items = r.json().get("items", [])
            cache_put(cpath, items); _sleep_with_jitter(NAVER_CALL_DELAY_SEC)
            return items
        except requests.RequestException as e:
            last_err = e
            wait = min(NAVER_BACKOFF_CAP, NAVER_BACKOFF_BASE * (2 ** (attempt - 1))); _sleep_with_jitter(wait)
    st.warning(f"ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œì´ ì¼ì‹œ ì œí•œë  ìˆ˜ ìˆì–´ìš”. (query='{query}')")
    return []

def extract_domain(u: str) -> str:
    if not u: return ""
    try:
        netloc = urlparse(u).netloc.lower()
        if netloc.startswith("www."): netloc = netloc[4:]
        return netloc
    except Exception: return ""

def count_in_windows_and_domains(items, now_kst: datetime, wmap: dict):
    c24 = c7 = c30 = 0
    c24_w = c7_w = 0.0
    domains_24h = set()
    for it in items:
        pub = it.get("pubDate"); link = it.get("originallink") or it.get("link")
        try:
            dt = dtparser.parse(pub)
            if not dt.tzinfo: dt = dt.replace(tzinfo=timezone.utc)
            dt = dt.astimezone(KST)
        except Exception:
            continue
        d = extract_domain(link); w = get_domain_weight(d, wmap)
        delta = now_kst - dt
        if delta <= timedelta(days=1):
            c24 += 1; c24_w += w
            if d: domains_24h.add(d)
        if delta <= timedelta(days=7):
            c7 += 1; c7_w += w
        if delta <= timedelta(days=30):
            c30 += 1
    return c24, c7, c30, len(domains_24h), round(c24_w,3), round(c7_w,3)

def spike_from_7d(c24: float, c7_total: float):
    baseline_per24 = max(1e-4, c7_total / 7.0)
    return c24 / baseline_per24

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GDELT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _gdelt_timeline_total(query: str, timespan: str, cache_hours: int = 6) -> int | None:
    key = cache_key(today_tag(), query, timespan); cpath = CACHE_GDELT / f"{key}.json"
    cached = cache_get(cpath, cache_hours)
    if cached is not None: return int(cached)
    url = f"https://api.gdeltproject.org/api/v2/doc/doc?query={quote(query)}&mode=TimelineVol&format=json&timespan={timespan}"
    for attempt in range(1, 3 + 1):
        try:
            r = SESSION.get(url, timeout=25)
            if r.status_code != 200: time.sleep(0.8*attempt); continue
            try: js = r.json()
            except Exception: time.sleep(0.8*attempt); continue
            timeline = js.get("timeline", [])
            if not timeline: cache_put(cpath, 0); return 0
            total = 0
            for b in timeline[0].get("data", []):
                try: total += int(b.get("value", 0))
                except Exception: pass
            cache_put(cpath, total); return total
        except requests.RequestException:
            time.sleep(0.8*attempt)
    return None

def _gdelt_timeline_series(query: str, timespan: str = "60d", cache_hours: int = 6) -> pd.Series:
    key = cache_key("series", today_tag(), query, timespan); cpath = CACHE_GDELT / f"{key}.json"
    cached = cache_get(cpath, cache_hours)
    if cached is not None:
        try:
            s = pd.Series(cached).astype(int); s.index = pd.to_datetime(s.index); return s
        except Exception: pass
    url = f"https://api.gdeltproject.org/api/v2/doc/doc?query={quote(query)}&mode=TimelineVol&format=json&timespan={timespan}"
    try:
        r = SESSION.get(url, timeout=25)
        if r.status_code != 200: return pd.Series(dtype=int)
        js = r.json(); timeline = js.get("timeline", [])
        if not timeline: cache_put(cpath, {}); return pd.Series(dtype=int)
        data = timeline[0].get("data", [])
        pairs = {}
        for b in data:
            try: pairs[b["date"]] = int(b.get("value", 0))
            except Exception: continue
        cache_put(cpath, pairs)
        s = pd.Series(pairs).astype(int); s.index = pd.to_datetime(s.index); return s
    except Exception:
        return pd.Series(dtype=int)

def gdelt_doc_count_smart(name_kr: str | None, name_en: str | None, timespan: str, cache_hours: int = 6) -> int:
    kr = (name_kr or "").strip(); en = (name_en or "").strip()
    candidates = []
    if en: candidates += [f"\"{en}\"", en]
    if kr: candidates += [f"\"{kr}\"", kr]
    for q in candidates:
        res = _gdelt_timeline_total(q, timespan, cache_hours=cache_hours)
        if res is not None: return int(res)
    return 0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í…Œë§ˆ ê·œì¹™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
THEME_RULES = {
    "2ì°¨ì „ì§€": ["2ì°¨ì „ì§€","ë°°í„°ë¦¬","ì–‘ê·¹ì¬","ìŒê·¹ì¬","ì „í•´ì§ˆ","LFP","NCM","ë¦¬íŠ¬","ì½”ë°œíŠ¸","ë‹ˆì¼ˆ","separator","cathode","anode"],
    "ì›ì „/SMR": ["ì›ì „","SMR","ì†Œí˜•ëª¨ë“ˆì›ìë¡œ","ê°€ì••ìˆ˜","ì›ìë ¥","ì‚¬ìš©í›„í•µì—°ë£Œ","ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","NuScale","Westinghouse"],
    "ë°˜ë„ì²´": ["ë°˜ë„ì²´","íŒŒìš´ë“œë¦¬","ë©”ëª¨ë¦¬","HBM","EUV","ASML","TSMC","fab","ì¹©"],
    "AI/ë°ì´í„°ì„¼í„°": ["AI","ì¸ê³µì§€ëŠ¥","ë°ì´í„°ì„¼í„°","GPU","ì—”ë¹„ë””ì•„","NVIDIA","LLM","ì„œë²„","í´ë¼ìš°ë“œ"],
    "ë¡œë´‡": ["ë¡œë´‡","AMR","AGV","ë¡œë³´í‹±ìŠ¤","í˜‘ë™ë¡œë´‡","actuator"],
    "ì „ê¸°ì°¨": ["ì „ê¸°ì°¨","EV","ì¶©ì „ì†Œ","ì´ˆê¸‰ì†","í…ŒìŠ¬ë¼","BYD"],
    "ë°©ì‚°": ["ë°©ì‚°","êµ°ìˆ˜","ë¯¸ì‚¬ì¼","K2","K9","FA-50","defense","íƒ„ì•½"],
    "ë°”ì´ì˜¤": ["ë°”ì´ì˜¤","ì„ìƒ","FDA","ì‹ ì•½","ì„¸í¬ì¹˜ë£Œ","mRNA","ì§„ë‹¨"],
    "í•´ìš´/ì¡°ì„ ": ["í•´ìš´","ìš´ì„","ì»¨í…Œì´ë„ˆ","ì¡°ì„ ","LNGì„ ","VLCC","ì„ ë°•"],
    "ê±´ì„¤/ì¸í”„ë¼": ["ê±´ì„¤","ì¸í”„ë¼","SOC","í† ëª©","ì›ê°€","ìˆ˜ì£¼","PF"],
    "í•­ê³µìš°ì£¼": ["í•­ê³µ","ìš°ì£¼","ìœ„ì„±","ë°œì‚¬ì²´","ë¡œì¼“","ì €ê¶¤ë„","SpaceX"],
    "í†µì‹ /ë„¤íŠ¸ì›Œí¬": ["5G","6G","ê´‘ì¼€ì´ë¸”","ë„¤íŠ¸ì›Œí¬","í†µì‹ ì‚¬","ìŠ¤ëª°ì…€"],
    "ì—ë„ˆì§€/ì •ìœ ": ["ì •ìœ ","ì„ìœ ","ê°€ìŠ¤","LNG","ì¬ìƒì—ë„ˆì§€","í’ë ¥","íƒœì–‘ê´‘"],
    "í¬í† ë¥˜/ìì›": ["í¬í† ë¥˜","ë‹ˆì˜¤ë””ë®´","í……ìŠ¤í…","êµ¬ë¦¬","ì² ê´‘ì„","ë¦¬íŠ¬ê´‘ì‚°"],
    "HVDC/ì „ë ¥": ["HVDC","ì´ˆê³ ì••","ë³€í™˜ì†Œ","ì¼€ì´ë¸”","ì „ë ¥ë§","ê³„í†µ"],
    "ë””ìŠ¤í”Œë ˆì´/ê´‘í•™": ["OLED","LCD","ë””ìŠ¤í”Œë ˆì´","ê´‘í•™","ë§ˆì´í¬ë¡œOLED","ê´‘í•™ë Œì¦ˆ"],
}

def classify_themes(texts: list[str], top_k: int = 3):
    score = {k:0 for k in THEME_RULES.keys()}
    joined = " ".join([t for t in texts if t])
    for theme, kws in THEME_RULES.items():
        s = 0
        for kw in kws:
            s += 2 * len(re.findall(rf"\b{re.escape(kw)}\b", joined, flags=re.IGNORECASE))
            s += 1 * joined.lower().count(kw.lower())
        score[theme] = s
    ranked = sorted(score.items(), key=lambda x: x[1], reverse=True)
    ranked = [t for t in ranked if t[1] > 0]
    return [t[0] for t in ranked[:top_k]] if ranked else []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ ì¹´ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False, ttl=2*60*60)
def extract_news_cards(query_display: str, limit: int = 10, cache_hours: int = 2):
    items = naver_news_search_all(query_display, max_items=limit, sort="date", cache_hours=cache_hours)
    cards = []
    for it in items[:limit]:
        title = strip_html(it.get("title","")); desc  = short_summary(it.get("description",""))
        link  = it.get("originallink") or it.get("link"); media = extract_domain(link)
        pub   = it.get("pubDate")
        try:
            dt = dtparser.parse(pub)
            if not dt.tzinfo: dt = dt.replace(tzinfo=timezone.utc)
            dt = dt.astimezone(KST); pub_str = dt.strftime("%m-%d %H:%M")
        except Exception: pub_str = ""
        cards.append({"time": pub_str, "media": media, "title": title, "summary": desc, "link": link})
    return cards

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì§‘ê³„(ê°€ì¤‘/í•„í„° ë°˜ì˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_cross_csv(
    entities, out_path: str,
    fast_mode: bool, gdelt_top_k: int, skip_gdelt: bool,
    min_unique_domains: int, naver_cache_h: int, gdelt_cache_h: int
) -> pd.DataFrame:
    now = datetime.now(KST); NEW_N = 3; rows = []
    wmap = load_domain_weights(DOMAIN_WEIGHTS_JSON)

    p1 = st.progress(0, text="1/2 NAVER ì§‘ê³„ ì¤‘...")
    total = len(entities)
    for idx, e in enumerate(entities, start=1):
        name_kr, name_en, disp = e["name_kr"], e["name_en"], e["display"]
        items = naver_news_search_all(disp, max_items=100, sort="date", cache_hours=naver_cache_h)
        n24, n7, n30, uniq24, n24_w, n7_w = count_in_windows_and_domains(items, now, wmap)
        spike_n = spike_from_7d(n24_w if n24_w>0 else n24, n7_w if n7_w>0 else n7)

        rows.append({
            "display": disp, "type": e["type"], "name_kr": name_kr, "name_en": name_en, "ticker": e.get("ticker",""),
            "naver_24h": n24, "naver_7d": n7, "naver_30d": n30,
            "naver_24h_weighted": n24_w, "naver_7d_weighted": n7_w,
            "naver_spike": round(spike_n, 3), "unique_domains_24h": uniq24,
            "gdelt_24h": 0, "gdelt_7d": 0, "gdelt_30d": 0, "gdelt_spike": 0.0,
            "combined_spike": round(spike_n, 3),
            "newly_appeared": (n30 <= 1 and n24 >= NEW_N)
        })
        if idx % 10 == 0: _sleep_with_jitter(NAVER_CALL_DELAY_SEC)
        p1.progress(min(int(idx/total*100), 100), text=f"1/2 NAVER ì§‘ê³„ ì¤‘... ({idx}/{total})")
    p1.empty()

    df = pd.DataFrame(rows)
    if min_unique_domains > 0:
        mask = df["unique_domains_24h"] >= min_unique_domains
        df.loc[~mask, ["combined_spike","naver_spike"]] = 0

    if skip_gdelt:
        df.to_csv(out_path, index=False, encoding="utf-8-sig"); return df

    # FAST: ìƒìœ„ Kë§Œ GDELT
    if fast_mode:
        shortlist = df.sort_values(["naver_spike","naver_24h"], ascending=[False, False]).head(gdelt_top_k)
        target_names = set(shortlist["display"])
    else:
        target_names = set(df["display"])

    p2 = st.progress(0, text="2/2 GDELT ì§‘ê³„ ì¤‘...")
    targets = [r for r in rows if r["display"] in target_names]
    total2 = len(targets)
    if total2 == 0:
        p2.empty()
        df = pd.DataFrame(rows)
        if min_unique_domains > 0:
            mask = df["unique_domains_24h"] >= min_unique_domains
            df.loc[~mask, ["combined_spike","naver_spike"]] = 0
        df.to_csv(out_path, index=False, encoding="utf-8-sig"); return df

    for jdx, r in enumerate(targets, start=1):
        g24 = gdelt_doc_count_smart(r["name_kr"], r["name_en"], "24h", cache_hours=gdelt_cache_h)
        g7  = gdelt_doc_count_smart(r["name_kr"], r["name_en"], "7d",  cache_hours=gdelt_cache_h)
        g30 = gdelt_doc_count_smart(r["name_kr"], r["name_en"], "30d", cache_hours=gdelt_cache_h)
        spike_g = spike_from_7d(g24, g7)
        r["gdelt_24h"], r["gdelt_7d"], r["gdelt_30d"], r["gdelt_spike"] = g24, g7, g30, round(spike_g,3)
        r["combined_spike"] = round(math.sqrt(max(r["naver_spike"],1e-4)*max(spike_g,1e-4)), 3) if (g24>0 or g7>0) else r["naver_spike"]
        r["newly_appeared"] = (r["naver_30d"] <= 1 and r["naver_24h"] >= 3) and (g30 <= 1)
        if jdx % 5 == 0: time.sleep(0.05)
        p2.progress(min(int(jdx/total2*100), 100), text=f"2/2 GDELT ì§‘ê³„ ì¤‘... ({jdx}/{total2})")
    p2.empty()

    df = pd.DataFrame(rows)
    if min_unique_domains > 0:
        mask = df["unique_domains_24h"] >= min_unique_domains
        df.loc[~mask, ["combined_spike","naver_spike"]] = 0

    df.to_csv(out_path, index=False, encoding="utf-8-sig"); return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‘œì‹œ/ì—°ì‚° ì•ˆì „ê°€ë“œ ìœ í‹¸ (NEW) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REQUIRED_NUMERIC = [
    "naver_spike","naver_24h","naver_7d","naver_30d",
    "gdelt_24h","gdelt_7d","gdelt_30d","combined_spike","unique_domains_24h",
    "naver_24h_weighted","naver_7d_weighted"
]
OPTIONAL_TEXT = ["display","ticker","name_kr","name_en","type","theme","aliases"]

def ensure_columns_and_types(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in OPTIONAL_TEXT:
        if c not in df.columns: df[c] = ""
        df[c] = df[c].astype(str).fillna("").str.strip()
    for c in REQUIRED_NUMERIC:
        if c not in df.columns: df[c] = 0
        else:
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)
    if "newly_appeared" not in df.columns: df["newly_appeared"] = False
    else:
        df["newly_appeared"] = df["newly_appeared"].astype(bool)
    return df

def quick_health(df: pd.DataFrame) -> dict:
    need = ["display","ticker","combined_spike","naver_spike","naver_24h","naver_7d","gdelt_24h"]
    out = {"rows": len(df), "missing": [c for c in need if c not in df.columns]}
    na_rate = df.isna().mean().sort_values(ascending=False)
    out["na_top5"] = na_rate.head(5).round(3).to_dict()
    for c in ["combined_spike","naver_spike","naver_24h","naver_7d","gdelt_24h"]:
        if c in df.columns:
            s = pd.to_numeric(df[c], errors="coerce").fillna(0)
            out[f"{c}_neg"] = int((s < 0).sum()); out[f"{c}_zeros"] = int((s == 0).sum())
    return out
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤ì‹œê°„ ì£¼ê°€ (ì•ˆì •í™”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_price_board(tickers: list[str]):
    if not yf or not tickers: return None
    rows = []; now_kst = datetime.now(KST).strftime("%m-%d %H:%M")
    for tk in tickers:
        price = chg = pct = None; ok = False
        trials = [("1mo","1d"), ("3mo","1d"), ("5d","60m"), ("5d","30m")]
        for period, interval in trials:
            try:
                hist = yf.Ticker(tk).history(period=period, interval=interval, auto_adjust=False)
                if hist is None or hist.empty: continue
                c = hist["Close"].dropna()
                if len(c)==0: continue
                last = float(c.iloc[-1]); prev = float(c.iloc[-2]) if len(c)>1 else last
                price = round(last,4); chg = round(last-prev,4); pct = round((chg/prev*100) if prev else 0.0, 2)
                ok = True; break
            except Exception: continue
        rows.append({"ticker": tk, "price": price, "change": chg, "pct": pct, "time": now_kst, "status": "OK" if ok else "EMPTY"})
    df = pd.DataFrame(rows)
    if (df["price"].isna()).all(): return None
    return df

# ====================== ë‰´ìŠ¤Ã—ê°€ê²© ì‹ í˜¸/ë°±í…ŒìŠ¤íŠ¸ ìœ í‹¸ ======================
@st.cache_data(show_spinner=False, ttl=2*60*60)
def build_news_daily_series(name_kr: str, name_en: str, display: str, days: int = 60,
                            bday_only: bool = True, holidays: set[date] | None = None) -> pd.Series:
    candidates = []
    if display: candidates.append(f"\"{display}\"")
    if name_en: candidates += [f"\"{name_en}\"", name_en]
    if name_kr: candidates += [f"\"{name_kr}\"", name_kr]
    for q in candidates:
        s = _gdelt_timeline_series(q, timespan=f"{days}d", cache_hours=6)
        if not s.empty:
            s = s.astype(int).sort_index()
            if bday_only:
                start = (datetime.now(KST) - timedelta(days=days)).date()
                end = datetime.now(KST).date()
                bidx = business_days_index(start, end, holidays or set())
                s = s.reindex(bidx, fill_value=0)
            else:
                s = s.asfreq("D").fillna(0)
            return s
    items = naver_news_search_all(display or name_kr or name_en, max_items=100, sort="date", cache_hours=2)
    counts = {}
    for it in items:
        pub = it.get("pubDate")
        try:
            dt = dtparser.parse(pub)
            if not dt.tzinfo: dt = dt.replace(tzinfo=timezone.utc)
            dt = dt.astimezone(KST).date()
            counts[str(dt)] = counts.get(str(dt), 0) + 1
        except Exception: continue
    if not counts: return pd.Series(dtype=int)
    s = pd.Series(counts); s.index = pd.to_datetime(s.index); s = s.sort_index()
    if bday_only:
        start = (datetime.now(KST) - timedelta(days=days)).date()
        end = datetime.now(KST).date()
        bidx = business_days_index(start, end, holidays or set())
        s = s.reindex(bidx, fill_value=0)
    else:
        s = s.asfreq("D").fillna(0)
    return s

def rolling_median_mad(s: pd.Series, win: int = 20):
    med = s.rolling(win, min_periods=max(2, win//2)).median()
    mad = s.rolling(win, min_periods=max(2, win//2)).apply(lambda x: np.median(np.abs(x - np.median(x))), raw=True)
    mad = mad.replace(0, np.nan)
    return med, mad

def news_zscore(news_count: pd.Series, win: int = 20) -> pd.Series:
    news_count = news_count.astype(float).fillna(0)
    med, mad = rolling_median_mad(news_count, win)
    z = (news_count - med) / mad
    return z.replace([np.inf, -np.inf], np.nan)

def compute_indicators(px: pd.DataFrame) -> pd.DataFrame:
    df = px.copy()
    if not isinstance(df.index, pd.DatetimeIndex): df.index = pd.to_datetime(df.index)
    df["ret"] = df["Close"].pct_change()
    df["SMA10"] = df["Close"].rolling(10).mean()
    df["SMA20"] = df["Close"].rolling(20).mean()
    hl = df["High"] - df["Low"]
    hc = (df["High"] - df["Close"].shift()).abs()
    lc = (df["Low"] - df["Close"].shift()).abs()
    tr = pd.concat([hl, hc, lc], axis=1).max(axis=1)
    df["ATR14"] = tr.rolling(14).mean()
    if "Volume" in df: df["VOL_SPike"] = df["Volume"] / df["Volume"].rolling(20).mean()
    if "Value" in df:  df["VAL_SPike"] = df["Value"] / df["Value"].rolling(20).mean()
    return df

@dataclass
class SignalParams:
    z_win: int = 20
    z_th: float = 3.0
    vol_mult: float = 1.5
    atr_stop: float = 2.0
    atr_tp: float = 4.0
    use_value: bool = False
    min_unique_domains: int = 3  # í—ˆìˆ˜ ë°©ì§€ í•„í„°

def generate_signals(px_df: pd.DataFrame,
                     news_daily_counts: pd.Series,
                     params: SignalParams,
                     mode: str = "breakout",
                     unique_domains_24h: int | None = None) -> pd.DataFrame:
    df = compute_indicators(px_df)
    df["NEWS_CNT"] = news_daily_counts.reindex(df.index).fillna(0)
    df["NEWS_Z"] = news_zscore(df["NEWS_CNT"], params.z_win)

    # í—ˆìˆ˜ ë°©ì§€
    if unique_domains_24h is not None and unique_domains_24h < params.min_unique_domains:
        df["BUY"], df["SELL"], df["reason"] = False, False, "blocked_low_unique_domains"
        df["entry_price"] = np.nan; df["stop_price"] = np.nan; df["take_price"] = np.nan
        df["strategy"] = mode
        return df

    # Spike ì‹œë¦¬ì¦ˆ ì•ˆì „ ìƒì„±
    spike = df["VAL_SPike"] if (params.use_value and "VAL_SPike" in df) else df.get("VOL_SPike")
    if spike is None:
        spike = pd.Series(0.0, index=df.index)
    else:
        spike = pd.to_numeric(spike, errors="coerce").fillna(0.0).astype(float)

    df["BUY"], df["SELL"], df["reason"] = False, False, ""

    if mode == "breakout":
        cond = ((df["NEWS_Z"] >= params.z_th) &
                (df["Close"] > df["Close"].rolling(20).max().shift(1)) &
                (spike >= params.vol_mult) &
                (df["Close"] >= df["SMA20"]))
        df.loc[cond, ["BUY","reason"]] = True, "news_z+breakout"

    elif mode == "pullback":
        y = df["NEWS_Z"].shift(1) >= params.z_th
        cond = y & (df["High"] > df["High"].shift(1)) & (df["Close"] >= df["SMA20"]) & (spike >= params.vol_mult)
        df.loc[cond, ["BUY","reason"]] = True, "news_pullback_break"

    elif mode == "drift":
        streak = (df["NEWS_Z"] > 0).rolling(3).sum() >= 3
        pos3 = (df["Close"].pct_change().rolling(3).sum() > 0)
        cond = streak & pos3 & (df["SMA10"] > df["SMA20"])
        df.loc[cond, ["BUY","reason"]] = True, "news_drift"

    df["entry_price"] = df["Open"].shift(-1).where(df["BUY"])  # ë‹¤ìŒ ìº”ë“¤ ì‹œê°€
    df["stop_price"]  = (df["entry_price"] - params.atr_stop * df["ATR14"]).where(df["BUY"])
    df["take_price"]  = (df["entry_price"] + params.atr_tp   * df["ATR14"]).where(df["BUY"])
    df["strategy"] = mode
    return df

def backtest(df: pd.DataFrame, slippage_bps: float = 3.0, commission_bps: float = 3.0):
    slip = float(slippage_bps) / 1e4
    comm = float(commission_bps) / 1e4
    trades = []
    for dt, r in df[df["BUY"]].iterrows():
        e = r.get("entry_price", np.nan)
        if pd.isna(e) or e == 0: continue
        e_cost = e * (1 + slip + comm)
        stop_p = r.get("stop_price", np.nan); take_p = r.get("take_price", np.nan)
        try: idx = df.index.get_loc(dt)
        except KeyError: continue
        nxt = idx + 1; outcome, exit_p = None, None
        if nxt < len(df.index):
            lo1 = df.iloc[nxt]["Low"]; hi1 = df.iloc[nxt]["High"]
            if not pd.isna(stop_p) and lo1 <= stop_p: exit_p, outcome = stop_p, "stop"
            if outcome is None and (not pd.isna(take_p)) and hi1 >= take_p: exit_p, outcome = take_p, "take"
        if exit_p is None: exit_p, outcome = df.at[dt, "Close"], "eod"
        x_cost = exit_p * (1 - slip - comm)
        ret = (x_cost - e_cost) / e_cost
        trades.append({"date": dt, "entry": float(e_cost), "exit": float(x_cost), "ret": float(ret),
                       "outcome": outcome, "reason": r.get("reason",""), "strategy": r.get("strategy","")})
    if not trades:
        stats = {"trades":0, "win_rate":np.nan, "avg_ret":np.nan, "cagr":np.nan,
                 "mdd":np.nan, "sharpe":np.nan, "take_ratio":np.nan, "stop_ratio":np.nan}
        return pd.DataFrame(), stats
    tdf = pd.DataFrame(trades).set_index("date").sort_index()
    eq = (1 + tdf["ret"]).cumprod(); peak = eq.cummax(); dd = eq/peak - 1
    ann = max(1, int(len(eq)/252))
    sr = tdf["ret"].mean() / (tdf["ret"].std() + 1e-9) * np.sqrt(252) if tdf["ret"].std() > 0 else np.nan
    stats = {
        "trades": int(len(tdf)),
        "win_rate": float((tdf["ret"]>0).mean()),
        "avg_ret": float(tdf["ret"].mean()),
        "cagr": float(eq.iloc[-1]**ann - 1),
        "mdd": float(dd.min()),
        "sharpe": float(sr),
        "take_ratio": float((tdf["outcome"]=="take").mean()),
        "stop_ratio": float((tdf["outcome"]=="stop").mean()),
    }
    return tdf, stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI (ìƒë‹¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ë‰´ìŠ¤ ì–¸ê¸‰ ê¸‰ë“± â€” í†µí•© ì§‘ê³„ & ëŒ€ì‹œë³´ë“œ (NAVER Ã— GDELT)")
st.caption("ë„ë©”ì¸ ê°€ì¤‘ ìŠ¤íŒŒì´í¬ + ê±°ë˜ì¼ ë³´ì • Z + í—ˆìˆ˜ ë°©ì§€ + ìŠ¬ë¦¬í”¼ì§€/ìˆ˜ìˆ˜ë£Œ + í”„ë¦¬ì…‹")
st.sidebar.caption(f"ğŸ”„ ë§ˆì§€ë§‰ ê°±ì‹ : {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S KST')}")

with st.sidebar:
    st.header("ì˜µì…˜")
    top_n = st.slider("Top N", 5, 50, 15, 1)
    max_entities = st.slider("ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬í•  ìµœëŒ€ ì—”í‹°í‹° ìˆ˜", 50, 3000, 300, 50)
    st.markdown("---")
    fast_mode = st.checkbox("FAST ëª¨ë“œ (ìƒìœ„ Kë§Œ GDELT)", value=True)
    gdelt_top_k = st.slider("FAST ëª¨ë“œì—ì„œ GDELT ê³„ì‚°í•  K", 10, 500, 100, 10, disabled=not fast_mode)
    skip_gdelt = st.checkbox("GDELT ì™„ì „ ê±´ë„ˆë›°ê¸° (NAVERë§Œ)", value=False)
    st.markdown("---")
    min_unique_domains = st.slider("ì •í™•ë„ í•„í„°: 24h ê³ ìœ  ë„ë©”ì¸ ìµœì†Œê°’", 0, 10, 3, 1)
    st.caption("â€» 3~5 ê¶Œì¥. í—ˆìˆ˜ ë‰´ìŠ¤ ì–µì œ")
    st.markdown("---")
    naver_cache_h = st.slider("NAVER ìºì‹œ ì‹œê°„(ì‹œê°„)", 1, 24, 6, 1)
    gdelt_cache_h = st.slider("GDELT ìºì‹œ ì‹œê°„(ì‹œê°„)", 1, 24, 6, 1)
    st.markdown("---")
    sort_choice = st.selectbox("ì •ë ¬ ê¸°ì¤€(í‘œ/ì°¨íŠ¸)", ["combined_spike","naver_spike","naver_24h","gdelt_24h"], index=0)
    st.markdown("---")
    st.write("ğŸ“„ ì—”í‹°í‹° ë¯¸ë¦¬ë³´ê¸° (entities.csv)")
    try:
        ents_df = pd.read_csv(ENTITIES_CSV, encoding="utf-8-sig")
    except Exception:
        try: ents_df = pd.read_csv(ENTITIES_CSV, encoding="cp949")
        except Exception: ents_df = None
    if ents_df is not None:
        st.dataframe(ents_df.head(200), use_container_width=True, hide_index=True)
    else:
        st.info("entities.csv ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.markdown("---")
    st.subheader("ìƒì„¸ ë³´ê¸° ì˜µì…˜")
    news_limit = st.slider("ê¸°ì‚¬ ìš”ì•½ ê°œìˆ˜", 3, 20, 10, 1)
    enable_themes = st.checkbox("AI í…Œë§ˆ ìë™ ë¶„ë¥˜ ë³´ê¸°", value=True)
    enable_prices = st.checkbox("ì‹¤ì‹œê°„ ì£¼ê°€ ë³´ë“œ í‘œì‹œ(í‹°ì»¤ ìˆëŠ” ì¢…ëª©ë§Œ)", value=True)
    protect_naver = st.checkbox("Naver API ë³´í˜¸ ëª¨ë“œ(í…Œë§ˆ ê°œìš” ë‰´ìŠ¤ í˜¸ì¶œ ìƒëµ)", value=False)
    st.markdown("---")
    # í—¬ìŠ¤ì²´í¬
    try:
        if os.path.exists(OUT_CSV):
            _tmp = pd.read_csv(OUT_CSV)
            st.sidebar.json({"health": quick_health(_tmp)})
    except Exception:
        pass

# â€˜ì²˜ìŒ ë“±ì¥â€™ ì¡°ê±´(ìœ ì—° ì¡°ì ˆ)
with st.sidebar.expander("â€˜ì²˜ìŒ ë“±ì¥â€™ ì¡°ê±´"):
    th_24h = st.slider("NAVER 24h â‰¥", 1, 10, 3, 1)
    th_30d = st.slider("NAVER 30d â‰¤", 0, 5, 1, 1)
    th_g30 = st.slider("GDELT 30d â‰¤", 0, 5, 1, 1)

st.write(f"**ê¸°ì¤€ ì‹œê°(KST)**: {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')}")

# ì§‘ê³„ ì‹¤í–‰/ë¡œë“œ
col1, col2 = st.columns([1,1])
with col1:
    if st.button("ğŸš€ ì§‘ê³„ ì‹¤í–‰ (CSV ìƒì„±/ê°±ì‹ )"):
        if not CLIENT_ID or not CLIENT_SECRET:
            st.error("`.env`ì˜ NAVER_CLIENT_ID / NAVER_CLIENT_SECRETê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("ì§‘ê³„ ì¤‘..."):
                entities = load_entities(ENTITIES_CSV)
                random.shuffle(entities); entities = entities[:max_entities]
                df = generate_cross_csv(
                    entities=entities, out_path=OUT_CSV,
                    fast_mode=fast_mode, gdelt_top_k=gdelt_top_k, skip_gdelt=skip_gdelt,
                    min_unique_domains=min_unique_domains, naver_cache_h=naver_cache_h, gdelt_cache_h=gdelt_cache_h,
                )
            st.success(f"CSV ì €ì¥ ì™„ë£Œ: {OUT_CSV} (í–‰ {len(df)})"); st.session_state["df"] = df

with col2:
    if st.button("ğŸ”„ CSV ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°"):
        if os.path.exists(OUT_CSV):
            st.session_state["df"] = pd.read_csv(OUT_CSV); st.success("CSV ë¡œë“œ ì™„ë£Œ")
        else:
            st.warning(f"{OUT_CSV}ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì§‘ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# ë°ì´í„° ì†ŒìŠ¤
df_src = None
if "df" in st.session_state: df_src = st.session_state["df"]
elif os.path.exists(OUT_CSV): df_src = pd.read_csv(OUT_CSV)
if df_src is None or df_src.empty:
    st.info("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € **ì§‘ê³„ ì‹¤í–‰**ì„ ëˆŒëŸ¬ CSVë¥¼ ìƒì„±í•˜ì„¸ìš”."); st.stop()

# â”€â”€ ì•ˆì „ê°€ë“œ ì ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df_base = ensure_columns_and_types(df_src)

# â”€â”€ ì •ë ¬/ë·° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if sort_choice not in df_base.columns: df_base[sort_choice] = 0
df_sorted = df_base.sort_values([sort_choice,"naver_24h","gdelt_24h"], ascending=[False,False,False]).reset_index(drop=True)

st.subheader("êµì°¨ ê¸‰ë“± TOP")
safe_cols = [c for c in [
    "display","ticker","combined_spike","naver_spike","naver_24h","naver_24h_weighted",
    "naver_7d","naver_7d_weighted","unique_domains_24h",
    "gdelt_24h","gdelt_7d","naver_30d","gdelt_30d","newly_appeared"
] if c in df_sorted.columns]
top_table = df_sorted.loc[:, safe_cols].head(top_n).copy()
for c in ["naver_24h_weighted","naver_7d_weighted"]:
    if c in top_table.columns:
        top_table[c] = top_table[c].replace(0, np.nan).fillna("â€“")
st.dataframe(top_table, use_container_width=True, hide_index=True)

st.subheader("ìƒìœ„ ì¢…ëª© ë§‰ëŒ€ì°¨íŠ¸ â€” " + sort_choice)
top = df_sorted.head(top_n).set_index("display")
st.bar_chart(top[sort_choice])

# Themes
if enable_themes:
    st.subheader("Themes Overview (AI ë¶„ë¥˜)")
    theme_acc = {}
    if not protect_naver:
        for disp in df_sorted.head(top_n)["display"].tolist():
            cards = extract_news_cards(disp, limit=5, cache_hours=2)
            texts = [c["title"] + " " + c["summary"] for c in cards]
            tms = classify_themes(texts, top_k=3)
            for t in tms: theme_acc[t] = theme_acc.get(t, 0) + 1
    if theme_acc:
        theme_df = pd.DataFrame([{"theme":k, "count":v} for k,v in sorted(theme_acc.items(), key=lambda x: x[1], reverse=True)])
        st.dataframe(theme_df, use_container_width=True, hide_index=True)
    else:
        st.write("í…Œë§ˆ ê°ì§€ ê²°ê³¼ ì—†ìŒ (ë˜ëŠ” ë³´í˜¸ ëª¨ë“œ)")

# â”€â”€ ìƒì„¸ ì¸ìŠ¤í™í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ìƒìœ„ ì¢…ëª© ìƒì„¸ ì¸ìŠ¤í™í„°")
left, right = st.columns([1,1])

with left:
    candidates = df_sorted.head(top_n)["display"].tolist()
    target_disp = st.selectbox("ì¢…ëª©/ê¸°ì—… ì„ íƒ", candidates)
    row = df_sorted[df_sorted["display"]==target_disp].head(1)
    ticker = (row["ticker"].iloc[0] if "ticker" in row.columns and len(row)>0 else "") or ""
    st.markdown(f"**ì„ íƒ:** {target_disp}" + (f"  |  **í‹°ì»¤:** `{ticker}`" if ticker else ""))

    st.markdown("##### ğŸ“° ìµœì‹  ê¸°ì‚¬ ìš”ì•½/ë§í¬")
    try: cards = extract_news_cards(target_disp, limit=news_limit, cache_hours=2)
    except Exception: cards = []
    if cards:
        news_df = pd.DataFrame(cards)
        news_df["link"] = news_df["link"].fillna("")  # ì•ˆì „ê°€ë“œ
        news_df = news_df[["time","media","title","summary","link"]]
        st.dataframe(news_df, use_container_width=True, hide_index=True,
                     column_config={"link": st.column_config.LinkColumn("link")})
    else:
        st.write("í‘œì‹œí•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if enable_themes and cards:
        texts = [c["title"] + " " + c["summary"] for c in cards]
        themes = classify_themes(texts, top_k=5)
        st.markdown("**AI ì¶”ì • í…Œë§ˆ:** " + (", ".join(themes) if themes else "(ì—†ìŒ)"))

with right:
    if enable_prices:
        st.markdown("##### ğŸ’¹ ì‹¤ì‹œê°„ ì£¼ê°€ ë³´ë“œ")
        tickers_top = [t for t in df_sorted.head(top_n)["ticker"].fillna("").tolist() if t]
        tickers_all = [t for t in df_sorted["ticker"].fillna("").tolist() if t]
        base_tickers = tickers_top if tickers_top else tickers_all[:20]
        if ticker: base_tickers = list({*base_tickers, ticker})
        manual = st.text_input("ìˆ˜ë™ í…ŒìŠ¤íŠ¸ í‹°ì»¤ ì¶”ê°€ (ì˜ˆ: AAPL, NVDA, 005930.KS)", value="")
        extra = [t.strip() for t in manual.split(",") if t.strip()]
        if extra: base_tickers = list({*base_tickers, *extra})
        if not yf:
            st.warning("yfinanceê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install yfinance` í›„ ì´ìš©í•˜ì„¸ìš”.")
        elif not base_tickers:
            st.info("í‘œì‹œí•  í‹°ì»¤ê°€ ì—†ìŠµë‹ˆë‹¤. CSVì˜ `ticker` ì»¬ëŸ¼ì„ ì±„ìš°ê±°ë‚˜, ìœ„ ì…ë ¥ë€ì— í…ŒìŠ¤íŠ¸ í‹°ì»¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            board = fetch_price_board(base_tickers)
            name_map = {}
            if "ticker" in df_sorted.columns:
                name_map = (df_sorted.loc[df_sorted["ticker"].fillna("")!="", ["ticker","display"]]
                                    .drop_duplicates(subset=["ticker"]).set_index("ticker")["display"].to_dict())
            if board is not None and not board.empty:
                board.insert(0, "name", board["ticker"].map(name_map).fillna(board["ticker"]))
                board = board[["name","ticker","price","change","pct","time","status"]].rename(columns={
                    "name":"íšŒì‚¬ëª…","ticker":"í‹°ì»¤","price":"ê°€ê²©","change":"ì „ì¼ëŒ€ë¹„","pct":"ë“±ë½ë¥ (%)","time":"ì‹œê°„","status":"ìƒíƒœ"
                })
            if board is not None and not board.empty and board["ê°€ê²©"].notna().any():
                st.dataframe(board, use_container_width=True, hide_index=True)
            else:
                st.error("ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì•¼í›„ í¬ë§· .KS/.KQ, ë°©í™”ë²½ ë“± í™•ì¸)")

        if yf and ticker:
            try:
                hist = yf.Ticker(ticker).history(period="1mo", interval="1d", auto_adjust=False)
                if hist is not None and not hist.empty:
                    disp_name = name_map.get(ticker, target_disp) if 'name_map' in locals() else target_disp
                    st.markdown(f"**{disp_name} ({ticker})** â€” 1M ì¢…ê°€")
                    st.line_chart(hist["Close"])
            except Exception:
                pass

# â”€â”€ ë§¤ìˆ˜ íƒ€ì´ë° â€” 3ì „ëµ í†µí•© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("---")
st.header("ğŸ“ˆ ë§¤ìˆ˜ íƒ€ì´ë° â€” 3ì „ëµ í†µí•©")

kr_holidays = load_kr_holidays(KR_HOLIDAY_CSV)

with st.sidebar.expander("ì‹ í˜¸ íŒŒë¼ë¯¸í„° (ê³µí†µ)", expanded=True):
    z_win = st.slider("ë‰´ìŠ¤ z ìœˆë„ìš°(ì˜ì—…ì¼)", 10, 40, 20, 1, key="z_win")
    z_th  = st.slider("ë‰´ìŠ¤ z ì„ê³„ì¹˜", 2.0, 6.0, 3.0, 0.1, key="z_th")
    vol_mult = st.slider("ê±°ë˜ëŸ‰/ëŒ€ê¸ˆ ë°°ìˆ˜", 1.0, 5.0, 1.5, 0.1, key="vol_mult")
    atr_stop = st.slider("ì†ì ˆ ATRë°°ìˆ˜", 1.0, 4.0, 2.0, 0.1, key="atr_stop")
    atr_tp   = st.slider("ìµì ˆ ATRë°°ìˆ˜", 2.0, 8.0, 4.0, 0.1, key="atr_tp")
    use_value = st.checkbox("ê±°ë˜ëŒ€ê¸ˆ(Value) ì‚¬ìš©", value=False, help="ê°€ê²© ë°ì´í„°ì— Value ì»¬ëŸ¼ ì¡´ì¬ ì‹œ", key="use_value")
    min_ud = st.slider("í—ˆìˆ˜ ë°©ì§€: ìµœì†Œ ê³ ìœ  ë„ë©”ì¸(24h)", 0, 10, min_unique_domains, 1, key="min_ud")

with st.sidebar.expander("ì „ëµ ê°€ì¤‘ì¹˜ + í”„ë¦¬ì…‹", expanded=True):
    def set_preset(kind: str):
        if kind == "normal":
            st.session_state["w_break"]=0.15; st.session_state["w_pull"]=0.25; st.session_state["w_drift"]=0.60
        elif kind == "bull":
            st.session_state["w_break"]=0.50; st.session_state["w_pull"]=0.10; st.session_state["w_drift"]=0.40
        elif kind == "choppy":
            st.session_state["w_break"]=0.10; st.session_state["w_pull"]=0.40; st.session_state["w_drift"]=0.50
    c1, c2, c3 = st.columns(3)
    with c1:
        if st.button("í‰ì‹œ í”„ë¦¬ì…‹"): set_preset("normal")
    with c2:
        if st.button("ê°•ì„¸ í”„ë¦¬ì…‹"): set_preset("bull")
    with c3:
        if st.button("ë³€ë™ í”„ë¦¬ì…‹"): set_preset("choppy")

    w_break = st.slider("ëŒíŒŒí˜• ê°€ì¤‘ì¹˜", 0.0, 1.0, st.session_state.get("w_break", 0.15), 0.05, key="w_break")
    w_pull  = st.slider("ë˜ëŒë¦¼í˜• ê°€ì¤‘ì¹˜", 0.0, 1.0, st.session_state.get("w_pull", 0.25), 0.05, key="w_pull")
    w_drift = st.slider("ë“œë¦¬í”„íŠ¸í˜• ê°€ì¤‘ì¹˜", 0.0, 1.0, st.session_state.get("w_drift", 0.60), 0.05, key="w_drift")

with st.sidebar.expander("ê±°ë˜ ì½”ìŠ¤íŠ¸(ìŠ¬ë¦¬í”¼ì§€Â·ìˆ˜ìˆ˜ë£Œ)", expanded=True):
    slip_bps = st.number_input("ìŠ¬ë¦¬í”¼ì§€ (bps, 0.01% = 1bps)", min_value=0.0, max_value=50.0, value=3.0, step=0.5, key="slip_bps")
    comm_bps = st.number_input("ì™•ë³µ ìˆ˜ìˆ˜ë£Œ (bps)", min_value=0.0, max_value=50.0, value=3.0, step=0.5, key="comm_bps")
    st.caption("ê¶Œì¥ ë²”ìœ„ 2~5 bps (0.02%~0.05%)")

# ì„ íƒ ì¢…ëª© ë‰´ìŠ¤/ê°€ê²©
try:
    name_kr = row["name_kr"].iloc[0] if "name_kr" in row.columns else target_disp
    name_en = row["name_en"].iloc[0] if "name_en" in row.columns else ""
    news_series = build_news_daily_series(name_kr=name_kr, name_en=name_en, display=target_disp, days=60,
                                          bday_only=True, holidays=kr_holidays)
    px_df = None
    if yf and ticker:
        px = yf.Ticker(ticker).history(period="6mo", interval="1d", auto_adjust=False)
        if px is not None and not px.empty:
            px_df = px.copy()[["Open","High","Low","Close","Volume"]]
            px_df.index = pd.to_datetime(px_df.index)
except Exception:
    news_series = pd.Series(dtype=int); px_df = None

# â˜…â˜…â˜… TZ íŒ¨ì¹˜ ì ìš©: ì¸ë±ìŠ¤ í†µì¼ í›„ reindex ì§„í–‰
if px_df is None or px_df.empty:
    st.info("âš ï¸ í‹°ì»¤ê°€ ì—†ëŠ” ì¢…ëª©ì…ë‹ˆë‹¤. í‹°ì»¤ê°€ ìˆëŠ” ì¢…ëª©ì„ ì„ íƒí•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")
else:
    px_df.index = to_naive_kst(px_df.index)
    if isinstance(news_series, pd.Series) and not news_series.empty:
        news_series.index = to_naive_kst(news_series.index)

    params = SignalParams(z_win=z_win, z_th=z_th, vol_mult=vol_mult, atr_stop=atr_stop, atr_tp=atr_tp,
                          use_value=use_value, min_unique_domains=min_ud)
    ud24 = int(row["unique_domains_24h"].iloc[0]) if "unique_domains_24h" in row.columns else None

    out = {}
    for mode in ["breakout","pullback","drift"]:
        sig_df = generate_signals(px_df, news_series, params, mode, unique_domains_24h=ud24)
        trades, stats = backtest(sig_df, slippage_bps=slip_bps, commission_bps=comm_bps)
        out[mode] = {"sig": sig_df, "trades": trades, "stats": stats}

    st.subheader(f"ì‹ í˜¸ ì„±ê³¼ ìš”ì•½ â€” {target_disp}" + (f" ({ticker})" if ticker else ""))

    def fmt_pct(x): return f"{x*100:.1f}%" if pd.notna(x) else "â€”"
    def fmt_float(x, n=2): return f"{x:.{n}f}" if pd.notna(x) else "â€”"

    cols = st.columns(4)
    for i, mode in enumerate(["breakout","pullback","drift"]):
        stats = out[mode]["stats"]
        with cols[i]:
            st.metric(f"{mode}", f"ìŠ¹ë¥  {fmt_pct(stats['win_rate'])}",
                      help=f"ê±°ë˜ìˆ˜ {stats['trades']} Â· Sharpe {fmt_float(stats['sharpe'])} Â· MDD {fmt_pct(stats['mdd'])}")

    def score(stats):
        if any(pd.isna(stats.get(k)) for k in ["win_rate","sharpe","mdd"]): return np.nan
        denom = 1 + abs(stats["mdd"]); return (stats["win_rate"] * max(stats["sharpe"], 0)) / denom

    sb, sp, sd = score(out["breakout"]["stats"]), score(out["pullback"]["stats"]), score(out["drift"]["stats"])
    blended = (st.session_state["w_break"]*(sb if pd.notna(sb) else 0) +
               st.session_state["w_pull"] *(sp if pd.notna(sp) else 0) +
               st.session_state["w_drift"]*(sd if pd.notna(sd) else 0))
    st.caption(f"ê°€ì¤‘ í†µí•© ì ìˆ˜(ì°¸ê³ ì¹˜): {fmt_float(blended, 3)}  Â·  ê°€ì¤‘ì¹˜ (ëŒíŒŒ {st.session_state['w_break']:.2f} / ë˜ëŒë¦¼ {st.session_state['w_pull']:.2f} / ë“œë¦¬í”„íŠ¸ {st.session_state['w_drift']:.2f})")

    st.markdown("#### ë‰´ìŠ¤ ì‹œê³„ì—´ / ë‰´ìŠ¤ Z / ë§¤ìˆ˜ ì‹ í˜¸")
    if isinstance(news_series, pd.Series) and not news_series.empty:
        nz = news_zscore(news_series, z_win).reindex(px_df.index, method="ffill")
        chart_df = pd.DataFrame({"NEWS_CNT": news_series.reindex(px_df.index).fillna(0), "NEWS_Z": nz})
        st.line_chart(chart_df)

    st.markdown("#### íŠ¸ë ˆì´ë“œ ë¡œê·¸")
    tabs = st.tabs(["ëŒíŒŒí˜•", "ë˜ëŒë¦¼í˜•", "ë“œë¦¬í”„íŠ¸í˜•"])
    for tab, mode in zip(tabs, ["breakout","pullback","drift"]):
        with tab:
            trades = out[mode]["trades"]; stats = out[mode]["stats"]
            st.write(f"ê±°ë˜ìˆ˜ {stats['trades']} Â· ìŠ¹ë¥  {fmt_pct(stats['win_rate'])} Â· í‰ê· ìˆ˜ìµ {fmt_pct(stats['avg_ret'])} Â· Sharpe {fmt_float(stats['sharpe'])} Â· MDD {fmt_pct(stats['mdd'])}")
            if trades is not None and not trades.empty:
                st.dataframe(trades.tail(50), use_container_width=True)
            else:
                block_msg = " (ê³ ìœ  ë„ë©”ì¸ ë¶€ì¡±ìœ¼ë¡œ ì°¨ë‹¨ë¨)" if ud24 is not None and ud24 < min_ud else ""
                st.info("ì‹ í˜¸ê°€ ì—†ê±°ë‚˜ ì²´ê²°ëœ íŠ¸ë ˆì´ë“œê°€ ì—†ìŠµë‹ˆë‹¤" + block_msg + ". íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.")

    st.markdown("#### ìµœê·¼ ì‹ í˜¸ ì‹œê°í™”(ê°„ë‹¨)")
    try:
        last_mode = st.selectbox("ì „ëµ ì„ íƒ", ["breakout","pullback","drift"], index=0)
        sig_df = out[last_mode]["sig"].copy()
        viz = pd.DataFrame({"Close": px_df["Close"]}); viz["BUY"] = sig_df["BUY"].astype(int)
        st.line_chart(viz[["Close","BUY"]])
    except Exception:
        pass

# â”€â”€ â€˜ì²˜ìŒ ë“±ì¥â€™ í›„ë³´ (df_base ê¸°ì¤€ ê³„ì‚°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("â€˜ì²˜ìŒ ë“±ì¥â€™ í›„ë³´")
tmp = df_base.copy()
for c in ["naver_24h","naver_30d","gdelt_30d","gdelt_24h"]:
    if c not in tmp.columns: tmp[c] = 0
    tmp[c] = pd.to_numeric(tmp[c], errors="coerce").fillna(0)

new_df = tmp[(tmp["naver_30d"] <= th_30d) & (tmp["naver_24h"] >= th_24h) & (tmp["gdelt_30d"] <= th_g30)]
new_df = new_df.sort_values(["naver_24h","gdelt_30d","naver_30d"], ascending=[False, True, True])

if new_df.empty:
    st.write(f"âšª í˜„ì¬ ì¡°ê±´(NAVER 30dâ‰¤{th_30d} & 24hâ‰¥{th_24h} AND GDELT 30dâ‰¤{th_g30})ì— í•´ë‹¹ ì—†ìŒ")
else:
    st.dataframe(
        new_df.loc[:, ["display","ticker","naver_24h","naver_30d","gdelt_24h","gdelt_30d","combined_spike"]].head(100),
        use_container_width=True, hide_index=True
    )
